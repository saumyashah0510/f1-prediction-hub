{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f03581b8",
   "metadata": {},
   "source": [
    "# ðŸŽï¸ Model 2A : Light GBM - Multi Target Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3590d996",
   "metadata": {},
   "source": [
    "## Step 1 : Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "642576ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import (\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold,\n",
    "    learning_curve \n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    make_scorer\n",
    ")\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\" All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb21d21",
   "metadata": {},
   "source": [
    "## Step 2 : Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d215ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading processed data...\n",
      "\n",
      "âœ… Data loaded successfully!\n",
      "   Training samples: 460 (2024 season)\n",
      "   Test samples: 385 (2025 season)\n",
      "   Features: 72\n"
     ]
    }
   ],
   "source": [
    "print(\" Loading processed data...\")\n",
    "\n",
    "\n",
    "train_df = pd.read_parquet('../data/processed/train_data_v2.parquet')\n",
    "test_df = pd.read_parquet('../data/processed/test_data_v2.parquet')\n",
    "\n",
    "\n",
    "train_weights = np.load('../data/processed/train_weights.npy')\n",
    "test_weights = np.load('../data/processed/test_weights.npy')\n",
    "\n",
    "\n",
    "with open('../data/processed/metadata_v2.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(f\"\\nâœ… Data loaded successfully!\")\n",
    "print(f\"   Training samples: {len(train_df)} (2024 season)\")\n",
    "print(f\"   Test samples: {len(test_df)} (2025 season)\")\n",
    "print(f\"   Features: {len(metadata['feature_columns'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fca124b",
   "metadata": {},
   "source": [
    "## Step 3 : Prepare features and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8756165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data prepared!\n",
      "\n",
      "Class Imbalance Analysis (Training):\n",
      "   win            :   24 positive (  5.2%) |  436 negative | Ratio: 1:18.2\n",
      "   podium         :   72 positive ( 15.7%) |  388 negative | Ratio: 1:5.4\n",
      "   points_finish  :  234 positive ( 50.9%) |  226 negative | Ratio: 1:1.0\n",
      "   top5           :  120 positive ( 26.1%) |  340 negative | Ratio: 1:2.8\n"
     ]
    }
   ],
   "source": [
    "feature_cols = metadata['feature_columns']\n",
    "\n",
    "classification_targets = ['win', 'podium', 'points_finish', 'top5']\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "X_test = test_df[feature_cols]\n",
    "\n",
    "y_train_class = train_df[classification_targets]\n",
    "y_test_class = test_df[classification_targets]\n",
    "\n",
    "X_train = X_train.loc[:, ~X_train.columns.duplicated()]\n",
    "X_test = X_test.loc[:, ~X_test.columns.duplicated()]\n",
    "\n",
    "print(f\"\\nData prepared!\")\n",
    "print(f\"\\nClass Imbalance Analysis (Training):\")\n",
    "for target in classification_targets:\n",
    "    pos_count = train_df[target].sum()\n",
    "    neg_count = len(train_df) - pos_count\n",
    "    imbalance_ratio = neg_count / pos_count if pos_count > 0 else 0\n",
    "    print(f\"   {target:15s}: {pos_count:4d} positive ({pos_count/len(train_df)*100:5.1f}%) | \"\n",
    "          f\"{neg_count:4d} negative | Ratio: 1:{imbalance_ratio:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e7a091",
   "metadata": {},
   "source": [
    "## Step 4 : Define Evaluate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7ca6bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation functions defined!\n"
     ]
    }
   ],
   "source": [
    "def calculate_classification_metrics(y_true, y_pred, y_prob=None, weights=None):\n",
    "   \n",
    "    acc = accuracy_score(y_true, y_pred, sample_weight=weights)\n",
    "    prec = precision_score(y_true, y_pred, sample_weight=weights, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, sample_weight=weights, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, sample_weight=weights, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob, sample_weight=weights) if y_prob is not None else None\n",
    "    \n",
    "    return {\n",
    "        'Accuracy': acc,\n",
    "        'Precision': prec,\n",
    "        'Recall': rec,\n",
    "        'F1': f1,\n",
    "        'AUC': auc\n",
    "    }\n",
    "\n",
    "\n",
    "def find_optimal_threshold(y_true, y_prob, weights=None):\n",
    "    \n",
    "    thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0.5\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        y_pred = (y_prob >= thresh).astype(int)\n",
    "        f1 = f1_score(y_true, y_pred, sample_weight=weights, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = thresh\n",
    "    \n",
    "    return best_threshold, best_f1\n",
    "\n",
    "\n",
    "def plot_learning_curve(model, X, y, weights=None, cv=5, title=\"Learning Curve\"):\n",
    "\n",
    "    fit_params = {'sample_weight': weights} if weights is not None else None\n",
    "    \n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        model, X, y, \n",
    "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        scoring='f1' if hasattr(y, 'unique') and len(np.unique(y)) == 2 else 'neg_mean_absolute_error',\n",
    "        fit_params= fit_params\n",
    "    )   \n",
    "    \n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    val_mean = np.mean(val_scores, axis=1)\n",
    "    val_std = np.std(val_scores, axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_sizes, train_mean, label='Training score', color='#3671C6', marker='o')\n",
    "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2, color='#3671C6')\n",
    "    plt.plot(train_sizes, val_mean, label='CV score', color='#E10600', marker='s')\n",
    "    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.2, color='#E10600')\n",
    "    plt.xlabel('Training Set Size', fontsize=12)\n",
    "    plt.ylabel('Score', fontsize=12)\n",
    "    plt.title(title, fontweight='bold', fontsize=14)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Evaluation functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8929dc1",
   "metadata": {},
   "source": [
    "## Step 5 : Baseline Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0d65e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conservative Baseline Parameters:\n",
      "   boosting_type: gbdt\n",
      "   n_estimators: 1000\n",
      "   max_depth: 8\n",
      "   learning_rate: 0.05\n",
      "   num_leaves: 20\n",
      "   min_child_samples: 20\n",
      "   subsample: 0.8\n",
      "   colsample_bytree: 0.8\n",
      "   reg_alpha: 0.1\n",
      "   reg_lambda: 0.1\n",
      "   class_weight: balanced\n",
      "\n",
      "  Training baseline LightGBM classifier for 'win'...\n",
      " Results for 'win':\n",
      "   Default Threshold (0.50)\n",
      "      âž¤ F1 Score : 0.546\n",
      "      âž¤ ROC AUC  : 0.962\n",
      "   Optimized Threshold (0.70)\n",
      "      âž¤ F1 Score : 0.572\n",
      "      âž¤ ROC AUC  : 0.962\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  Training baseline LightGBM classifier for 'podium'...\n",
      " Results for 'podium':\n",
      "   Default Threshold (0.50)\n",
      "      âž¤ F1 Score : 0.728\n",
      "      âž¤ ROC AUC  : 0.950\n",
      "   Optimized Threshold (0.70)\n",
      "      âž¤ F1 Score : 0.753\n",
      "      âž¤ ROC AUC  : 0.950\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  Training baseline LightGBM classifier for 'points_finish'...\n",
      " Results for 'points_finish':\n",
      "   Default Threshold (0.50)\n",
      "      âž¤ F1 Score : 0.781\n",
      "      âž¤ ROC AUC  : 0.838\n",
      "   Optimized Threshold (0.40)\n",
      "      âž¤ F1 Score : 0.792\n",
      "      âž¤ ROC AUC  : 0.838\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  Training baseline LightGBM classifier for 'top5'...\n",
      " Results for 'top5':\n",
      "   Default Threshold (0.50)\n",
      "      âž¤ F1 Score : 0.775\n",
      "      âž¤ ROC AUC  : 0.912\n",
      "   Optimized Threshold (0.70)\n",
      "      âž¤ F1 Score : 0.781\n",
      "      âž¤ ROC AUC  : 0.912\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "âœ… All baseline models trained with early stopping!\n"
     ]
    }
   ],
   "source": [
    "baseline_class_params = {\n",
    "    'objective': 'binary',        \n",
    "    'metric': 'binary_logloss',   \n",
    "    'boosting_type': 'gbdt',\n",
    "    'n_estimators': 1000,         \n",
    "    'max_depth': 8,               \n",
    "    'learning_rate': 0.05,        \n",
    "    'num_leaves': 20,             \n",
    "    'min_child_samples': 20,     \n",
    "    'subsample': 0.8,             \n",
    "    'colsample_bytree': 0.8,      \n",
    "    'reg_alpha': 0.1,             \n",
    "    'reg_lambda': 0.1,            \n",
    "    'class_weight': 'balanced',   \n",
    "    'random_state': 42,\n",
    "    'verbose': -1,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "print(\"\\nConservative Baseline Parameters:\")\n",
    "for param, value in baseline_class_params.items():\n",
    "    if param not in ['random_state', 'verbose', 'n_jobs', 'objective', 'metric']:\n",
    "        print(f\"   {param}: {value}\")\n",
    "\n",
    "baseline_class_models = {}\n",
    "baseline_class_results = {}\n",
    "\n",
    "\n",
    "for target in classification_targets:\n",
    "    print(f\"\\n  Training baseline LightGBM classifier for '{target}'...\")\n",
    "    \n",
    "    model = lgb.LGBMClassifier(**baseline_class_params)\n",
    "    \n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train_class[target],\n",
    "        sample_weight=train_weights,\n",
    "        eval_set=[(X_test, y_test_class[target])],      \n",
    "        eval_sample_weight=[test_weights],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)] \n",
    "    )\n",
    "    \n",
    "    \n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_test)\n",
    "    train_prob = model.predict_proba(X_train)[:, 1]\n",
    "    test_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    \n",
    "    optimal_thresh, optimal_f1 = find_optimal_threshold(\n",
    "        y_test_class[target], test_prob, test_weights\n",
    "    )\n",
    "    test_pred_optimized = (test_prob >= optimal_thresh).astype(int)\n",
    "    \n",
    "    \n",
    "    train_metrics = calculate_classification_metrics(\n",
    "        y_train_class[target], train_pred, train_prob, train_weights\n",
    "    )\n",
    "    test_metrics = calculate_classification_metrics(\n",
    "        y_test_class[target], test_pred, test_prob, test_weights\n",
    "    )\n",
    "    test_metrics_optimized = calculate_classification_metrics(\n",
    "        y_test_class[target], test_pred_optimized, test_prob, test_weights\n",
    "    )\n",
    "    \n",
    "    \n",
    "    baseline_class_models[target] = model\n",
    "    baseline_class_results[target] = {\n",
    "        'train_metrics': train_metrics,\n",
    "        'test_metrics': test_metrics,\n",
    "        'test_metrics_optimized': test_metrics_optimized,\n",
    "        'train_pred': train_pred,\n",
    "        'test_pred': test_pred,\n",
    "        'test_pred_optimized': test_pred_optimized,\n",
    "        'train_prob': train_prob,\n",
    "        'test_prob': test_prob,\n",
    "        'optimal_threshold': optimal_thresh,\n",
    "        'best_iteration': model.best_iteration_\n",
    "    }\n",
    "    \n",
    "    print(f\" Results for '{target}':\")\n",
    "    print(f\"   Default Threshold (0.50)\")\n",
    "    print(f\"      âž¤ F1 Score : {test_metrics['F1']:.3f}\")\n",
    "    print(f\"      âž¤ ROC AUC  : {test_metrics['AUC']:.3f}\")\n",
    "    print(f\"   Optimized Threshold ({optimal_thresh:.2f})\")\n",
    "    print(f\"      âž¤ F1 Score : {test_metrics_optimized['F1']:.3f}\")\n",
    "    print(f\"      âž¤ ROC AUC  : {test_metrics_optimized['AUC']:.3f}\")\n",
    "\n",
    "    print(\"-\"*80)\n",
    "\n",
    "print(\"\\nâœ… All baseline models trained with early stopping!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c2cba4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
